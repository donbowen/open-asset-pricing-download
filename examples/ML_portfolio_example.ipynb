{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88102421-a112-4104-a500-9994fed738d2",
   "metadata": {},
   "source": [
    "# Machine Learning Portfolio Example\n",
    "\n",
    "This example illustrates how to use all Chen-Zimmermann predictors, together with CRSP data. We'll merge monthly CRSP with the full set of Chen-Zimmermann predictors, fit the CRSP returns to lagged signals, and form portfolios in a super simple out-of-sample test. Specifically, we'll use a \"groovy\" model (fit on the 1960s and 1970s) to try to predict returns during hair metal (1980s), gangsta rap (1990s), and other more recent samples. Does the groovy model work even in the TSwift era?\n",
    "\n",
    "Downloading all of the signals takes some time and requires substantial RAM. It also requires a WRDS account, since some predictors require data from WRDS (size, short-term reversal, price)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272dc51-8f5a-4280-a3bc-6e3cb4110bd5",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-weight:bold\">Note</span>\n",
    "\n",
    "This example is an alternative version of *ML_portfolio_example.ipynb*. We prepared this example in case your computer has limited memory. The dataset used in this example is larger than 8GB, and Pandas DataFrame may struggle to process it if your computer does not have sufficient memory.\n",
    " \n",
    "To address this, we use Polars DataFrame and set signal variables to float32 (instead of float64) to reduce memory usage.\n",
    "\n",
    "The code should run without memory errors if your system has at least **16GB** of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4767164-0056-4cb3-a1c2-b1c3c7289b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import openassetpricing as oap\n",
    "import numpy as np\n",
    "import wrds\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize OpenAP\n",
    "openap = oap.OpenAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d07ff-4ec0-496b-9883-0a5ed96260c1",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "You'll have to enter your WRDS credentials twice: once to download the CRSP returns, and once to download all Chen-Zimmermann predictors (including size, short-term reversal, and price). The downloads take a couple minutes in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e0753-c465-4cb3-8440-7fdf95d4e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds_conn = wrds.Connection()\n",
    "\n",
    "crsp = wrds_conn.raw_sql(\n",
    "    \"\"\"select permno, date, ret*100 as ret\n",
    "                        from crsp.msf\"\"\",\n",
    "    date_cols=[\"date\"],\n",
    ")\n",
    "\n",
    "crsp = pl.from_pandas(crsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1acb9-e7bc-441a-9e98-802be91e10d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download all Chen-Zimmermann predictors\n",
    "bigdat = openap.dl_all_signals(\"polars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90776261-5522-4485-864f-6cd4e63e6b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get names of all signals\n",
    "signal_list = [col for col in bigdat.columns if col not in [\"permno\", \"yyyymm\"]]\n",
    "\n",
    "bigdat = bigdat.with_columns(pl.col(signal_list).cast(pl.Float32))\n",
    "\n",
    "bigdat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb1200-1b8c-4aab-809f-078fed8c4c9d",
   "metadata": {},
   "source": [
    "# Lag signals and merge\n",
    "\n",
    "To lag signals, you can just add one month to the `yyyymm` column for the signals. For simplicity, let's fill in the day of the new variable `date` as the 28th (the signals are assumed to be available for trading at the end of the month). You can keep around `yyyymm` as `yyyymm_signals` for sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39970f8-66ac-4518-b863-939800337670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigdat = bigdat.select(\n",
    "    \"permno\",\n",
    "    # Create date that is one month ahead for merging with returns\n",
    "    pl.col(\"yyyymm\")\n",
    "    .cast(pl.String)\n",
    "    .add(\"28\")\n",
    "    .str.to_date(\"%Y%m%d\")\n",
    "    .dt.offset_by(\"1mo\")\n",
    "    .alias(\"date\"),\n",
    "    # rename yyyymm for clarity\n",
    "    pl.col(\"yyyymm\").alias(\"yyyymm_signals\"),\n",
    "    pl.col(signal_list),\n",
    ")\n",
    "\n",
    "bigdat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151ecac-6c80-4b80-9c72-65a512890d3b",
   "metadata": {},
   "source": [
    "Now merge with CRSP. Convert CRSP dates to the 28th of the month for simplicity. The left join makes the missing values issues transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589bed7-adcd-4492-81ee-ca1f1d1a8e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert crsp dates to the 28th of the month\n",
    "crsp = crsp.select(\n",
    "    pl.col(\"permno\").cast(pl.Int32),\n",
    "    pl.col(\"date\")\n",
    "    .dt.year()\n",
    "    .mul(100)\n",
    "    .add(pl.col(\"date\").dt.month())\n",
    "    .cast(pl.String)\n",
    "    .add(\"28\")\n",
    "    .str.to_date(\"%Y%m%d\"),\n",
    "    \"ret\",\n",
    ")\n",
    "\n",
    "# lLeft join returns onto signals, in-place (for ram)\n",
    "bigdat = crsp.join(bigdat, on=[\"permno\", \"date\"], how=\"left\")\n",
    "\n",
    "bigdat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a038aa-fef3-4c5d-ac5e-5c15b4cf8f42",
   "metadata": {},
   "source": [
    "Congrats, the data is merged! But unfortunately, we'll need to do a bit more work to make it usable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def6e12-1f2e-4ac9-a942-eed0c5ed70d5",
   "metadata": {},
   "source": [
    "# Process data\n",
    "We'll need to deal with the missing signals. This is a notorious issue with big data. Here, we'll just standardize the signals and then fill in missings with zero. This follows [Chen and McCoy (2024)](https://arxiv.org/abs/2207.13071)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b5cd5d-0ece-44b0-a4ce-002e3386c204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy over, keep only after 1963 and non-missing returns\n",
    "cleandat = bigdat.filter(pl.col(\"date\").dt.year() >= 1963, pl.col(\"ret\").is_not_null())\n",
    "\n",
    "cleandat = (\n",
    "    cleandat\n",
    "    # Standardize\n",
    "    .with_columns(\n",
    "        (pl.col(signal_list) - pl.col(signal_list).mean()) / pl.col(signal_list).std()\n",
    "    )\n",
    "    # Replace null with 0\n",
    "    .fill_null(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285f386",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-weight:bold\">Optional:</span>\n",
    "To avoid issues when running the following examples on machines with lower memory (< 16GB), let's add an optional step to only use the first 20 predictors by alphabetical order. <span style=\"color:green;\">If you are using a computer with more than 16GB of RAM, you can skip this step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional step:\n",
    "cleandat = cleandat.select(cleandat.columns[:24])\n",
    "# Make our signal list match our selected data\n",
    "signal_list = signal_list[:20]\n",
    "\n",
    "cleandat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce887d-e653-40b3-8bab-376a783517fa",
   "metadata": {},
   "source": [
    "# Form ML-style portfolios\n",
    "Following Lewellen (2014, CFR), let's predict returns using many signals and then sort stocks on the predicted returns. We'll do this in perhaps the simplest way possible: fit returns with OLS using the \"groovy\" 1963-1979 sample. Then use the fitted coefficients on lagged signals to sort stocks every month from 1980 onward. This can't work, can it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90706acc-f2fc-446c-ab9c-c89898f9ebf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User-specified fit period\n",
    "fit_start = 1963\n",
    "fit_end = 1979\n",
    "\n",
    "# User-specified number of portfolios\n",
    "nport = 5\n",
    "\n",
    "# Make a copy of our dataframe specific to the OLS example\n",
    "cleandat_ols = cleandat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e16efca4-de8b-458f-8cc7-b086e082e458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit returns\n",
    "formula = \"ret ~ \" + \" + \".join(signal_list)\n",
    "\n",
    "fit = smf.ols(\n",
    "    formula,\n",
    "    data=cleandat_ols.filter(\n",
    "        pl.col(\"date\").dt.year().is_between(fit_start, fit_end - 1)\n",
    "    ),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f7f894-38e3-4607-a0a1-f62934ed0b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply fit to all data\n",
    "# Do it chunk by chunk to avoid large momory consumption caused by large matrix operation\n",
    "res = []\n",
    "for i in cleandat_ols.iter_slices(n_rows=len(cleandat_ols) // 100):\n",
    "    temp = i.select(pl.lit(1), pl.col(signal_list)).to_numpy() @ fit.params.values\n",
    "    res += list(temp)\n",
    "\n",
    "cleandat_ols = cleandat_ols.with_columns(pred=np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cd63567-ff24-4dff-850c-1e08f94397c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Find portfolio returns ==\n",
    "\n",
    "# Copy data\n",
    "preddat = cleandat_ols.select(\"permno\", \"date\", \"pred\", \"ret\").to_pandas()\n",
    "\n",
    "\n",
    "# Define port sort function\n",
    "# Follows https://github.com/chenandrewy/flex-mining/blob/70ca658090a13fea8517945280b2de83b9886968/0_Environment.R#L465\n",
    "def port_sort(x, nport):\n",
    "    return np.ceil(x.rank(method=\"min\") * nport / (len(x) + 1)).astype(int)\n",
    "\n",
    "\n",
    "preddat[\"port\"] = preddat.groupby(\"date\")[\"pred\"].transform(port_sort, nport=nport)\n",
    "\n",
    "# Find portfolio returns\n",
    "portdat = (\n",
    "    preddat.groupby([\"port\", \"date\"], observed=False)\n",
    "    .agg(ret=(\"ret\", \"mean\"), nstock=(\"permno\", \"nunique\"))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3e5e9",
   "metadata": {},
   "source": [
    "# Far Out-of-Sample Performance\n",
    "Let's examine the performance of our groovy model, into the hair metal (1980s), gangsta rap (1990s), emo (2000s), EDM (2010s), and TSwift (2020s) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfc154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find performance by 10-year periods\n",
    "samplength = 10\n",
    "\n",
    "portdat[\"subsamp\"] = pd.cut(\n",
    "    portdat[\"date\"].dt.year,\n",
    "    bins=range(1959, 2030, samplength),\n",
    "    labels=range(1959, 2029, samplength),\n",
    ")\n",
    "\n",
    "portsum = (\n",
    "    portdat.groupby([\"port\", \"subsamp\"], observed=False)\n",
    "    .agg(\n",
    "        meanret=(\"ret\", \"mean\"),\n",
    "        vol=(\"ret\", \"std\"),\n",
    "        nmonth=(\"date\", \"nunique\"),\n",
    "        nstock=(\"nstock\", \"mean\"),\n",
    "        datemin=(\"date\", \"min\"),\n",
    "        datemax=(\"date\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "portsum[\"meanret\"] = round(portsum[\"meanret\"], 2)\n",
    "\n",
    "# Pivot meanret to wide format\n",
    "sumwide = portsum.pivot(index=\"subsamp\", columns=\"port\", values=\"meanret\").reset_index()\n",
    "sumwide.columns = [\"subsamp\"] + [f\"port_{col}\" for col in sumwide.columns[1:]]\n",
    "\n",
    "# Add long-short\n",
    "sumwide[\"5_minus_1\"] = sumwide[\"port_5\"] - sumwide[\"port_1\"]\n",
    "\n",
    "# Add date ranges\n",
    "temp = (\n",
    "    portsum.groupby(\"subsamp\", observed=False)\n",
    "    .agg(datemin=(\"datemin\", \"min\"), datemax=(\"datemax\", \"max\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "sumwide = pd.merge(temp, sumwide, on=\"subsamp\", how=\"left\")\n",
    "\n",
    "# Name the subsamples\n",
    "sumwide[\"subsamp\"] = sumwide[\"subsamp\"].map(\n",
    "    {\n",
    "        1959: \"groovy\",\n",
    "        1969: \"groovy (still)\",\n",
    "        1979: \"hair metal\",\n",
    "        1989: \"gangsta rap\",\n",
    "        1999: \"emo\",\n",
    "        2009: \"EDM\",\n",
    "        2019: \"TSwift\",\n",
    "    }\n",
    ")\n",
    "\n",
    "sumwide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373b20c",
   "metadata": {},
   "source": [
    "The model, fit only using groovy era data, makes it through hair metal, gansta rap, and emo quite well. In the corresponding decades, the groovy model earns long-short returns of 2.0 to 3.0 percent per month. So a model from the [Simon and Garfunkel](https://en.wikipedia.org/wiki/Groovy#/media/File:Soundofsilence.jpg) days continued to predict quite well, even while [Metallica inexplicably started to paint their fingernails black](https://www.reddit.com/r/Metallica/comments/huk18i/never_forget_emotallica/). During EDM and the Tswift eras, the model predicts with the some notable magnitudes, though the returns are much weaker than they were while [Ms. Swift was still into pickup trucks](https://www.youtube.com/watch?v=GkD20ajVxnY).\n",
    "\n",
    "There are huge caveats about trading costs (Chen and Velikov 2023). But then again, this tutorial doesn't even attempt to deal with trading costs. One can likely do much better by following DeMiguel, Martin-Utrera, Nogales, and Uppal (2020) or Jensen, Kelly, Malamud, and Pedersen (2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2fd96",
   "metadata": {},
   "source": [
    "# 3-Layer Neural Network Using scikit-learn\n",
    "\n",
    "Let's replicate the steps above forming an ML-style portfolio, now with scikit-learn's implementation of a 3-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "241ad656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameters\n",
    "fit_start = 1963\n",
    "fit_end = 1979\n",
    "nport = 5\n",
    "\n",
    "cleandat_mlp = cleandat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf25e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "filtered_data = cleandat_mlp.filter(\n",
    "    pl.col(\"date\").dt.year().is_between(fit_start, fit_end - 1)\n",
    ")\n",
    "X = filtered_data.select(pl.col(signal_list)).to_pandas().values\n",
    "y = filtered_data.select(pl.col(\"ret\")).to_pandas().values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4272c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Define and train the MLPRegressor\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(32, 16, 8),  # Increase neurons for more learning capacity\n",
    "    activation=\"relu\",  # Good for nonlinear relationships\n",
    "    solver=\"adam\",  # Robust optimizer for noisy data\n",
    "    alpha=0.001,  # Add regularization to reduce overfitting\n",
    "    batch_size=10000,\n",
    "    learning_rate_init=0.01,  # Lower learning rate for finer optimization\n",
    "    max_iter=100,  # Allow more iterations for convergence\n",
    "    early_stopping=True,  # Stop training if validation error doesn't improve\n",
    "    n_iter_no_change=5,  # Patience for early stopping\n",
    "    random_state=1,  # Ensure reproducibility\n",
    ")\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b5a254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the trained model to all data (chunk by chunk)\n",
    "res = []\n",
    "for i in cleandat_mlp.iter_slices(n_rows=len(cleandat_mlp) // 100):\n",
    "    temp_data = i.select(pl.col(signal_list)).to_numpy()\n",
    "    temp_data_scaled = scaler.transform(temp_data)\n",
    "    temp_mlp = mlp.predict(temp_data_scaled)\n",
    "    res.extend(temp_mlp)\n",
    "\n",
    "cleandat_mlp = cleandat_mlp.with_columns(pred=np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57316738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Find portfolio returns ==\n",
    "# Copy data\n",
    "preddat_mlp = cleandat_mlp.select(\"permno\", \"date\", \"pred\", \"ret\").to_pandas()\n",
    "\n",
    "# Define port sort function\n",
    "def port_sort(x, nport):\n",
    "    return np.ceil(x.rank(method=\"min\") * nport / (len(x) + 1)).astype(int)\n",
    "\n",
    "preddat_mlp[\"port\"] = preddat_mlp.groupby(\"date\")[\"pred\"].transform(\n",
    "    port_sort, nport=nport\n",
    ")\n",
    "\n",
    "# Find portfolio returns\n",
    "portdat_mlp = (\n",
    "    preddat_mlp.groupby([\"port\", \"date\"], observed=False)\n",
    "    .agg(ret=(\"ret\", \"mean\"), nstock=(\"permno\", \"nunique\"))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d723f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find performance by 10-year periods\n",
    "samplength = 10\n",
    "\n",
    "portdat_mlp[\"subsamp\"] = pd.cut(\n",
    "    portdat_mlp[\"date\"].dt.year,\n",
    "    bins=range(1959, 2030, samplength),\n",
    "    labels=range(1959, 2029, samplength),\n",
    ")\n",
    "\n",
    "portsum_mlp = (\n",
    "    portdat_mlp.groupby([\"port\", \"subsamp\"], observed=False)\n",
    "    .agg(\n",
    "        meanret=(\"ret\", \"mean\"),\n",
    "        vol=(\"ret\", \"std\"),\n",
    "        nmonth=(\"date\", \"nunique\"),\n",
    "        nstock=(\"nstock\", \"mean\"),\n",
    "        datemin=(\"date\", \"min\"),\n",
    "        datemax=(\"date\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "portsum_mlp[\"meanret\"] = round(portsum_mlp[\"meanret\"], 2)\n",
    "\n",
    "# Pivot meanret to wide format\n",
    "sumwide_mlp = portsum_mlp.pivot(\n",
    "    index=\"subsamp\", columns=\"port\", values=\"meanret\"\n",
    ").reset_index()\n",
    "sumwide_mlp.columns = [\"subsamp\"] + [f\"port_{col}\" for col in sumwide_mlp.columns[1:]]\n",
    "\n",
    "# Add long-short\n",
    "sumwide_mlp[\"5_minus_1\"] = sumwide_mlp[\"port_5\"] - sumwide_mlp[\"port_1\"]\n",
    "\n",
    "# Add date ranges\n",
    "temp_mlp = (\n",
    "    portsum_mlp.groupby(\"subsamp\", observed=False)\n",
    "    .agg(datemin=(\"datemin\", \"min\"), datemax=(\"datemax\", \"max\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "sumwide_mlp = pd.merge(temp_mlp, sumwide_mlp, on=\"subsamp\", how=\"left\")\n",
    "\n",
    "# Name the subsamples\n",
    "sumwide_mlp[\"subsamp\"] = sumwide_mlp[\"subsamp\"].map(\n",
    "    {\n",
    "        1959: \"groovy\",\n",
    "        1969: \"groovy (still)\",\n",
    "        1979: \"hair metal\",\n",
    "        1989: \"gangsta rap\",\n",
    "        1999: \"emo\",\n",
    "        2009: \"EDM\",\n",
    "        2019: \"TSwift\",\n",
    "    }\n",
    ")\n",
    "\n",
    "sumwide_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a28d6d",
   "metadata": {},
   "source": [
    "It turns out that our simple neural network model actually performs worse than the OLS model. This should not be too surprising as our model is quite basic. Because of its simplicity, we can take this a step further by refitting our model using expanding windows to address some of the limitations in the initial training process as done in [Gu, Kelly, Xiu](https://download.ssrn.com/19/09/13/ssrn_id3453437_code759326.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEND%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCgu7K77DNk8hweLAjSBPgiEHKVleZYTKEaLgWWjsc0egIhAIrF6DeoeCccwZDKlP5OG65qbx2rh8UvM%2FyTSGxxI%2BQ5KscFCMj%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBBoMMzA4NDc1MzAxMjU3IgwpxDgEagk5mJZyCE4qmwWHzm2zDPI4UfEq5RH46Red%2FEp9tWn4z2Yjmy8lPi9kCdKNsgoRSijMvt1H1rcspbyMrgV7emiPp6UjWPW86K0J80HzFtOklEM%2FzPAd93z03o3Ise0Rio44NgVr4SEsN5bfVjk9LZZYgFAstCUgA96yGK%2BbS6GjTZTfyKZJX6AIn2%2F7i7o2ddsbdd1T0xxIa0fCVcWeE12N52Ls3AEP8NaMH22eCXBedF%2FAUJnx6OcuZw4Z8VsxtaOePQdWxwg%2FtB%2B5TrhPg1pZB16QkkwTAUlElCmGzLYfGkZ3laq5i3oaumlolRJkyS3s%2Bl%2FvnutFugngHabDDdpxemzvmv5jFzPQxYBLCpGWXyGnHTrZlsy1TkdjwIxH1iWn1ovgmREkACGvKMFTH0d57jYGA%2F3iA0F1xHSQu5%2BDgzwEEW2IhsY0U6t4%2BzOVjva7EonsQVFQ7PUyAvoaxglpCS0Fp6VhOJryFcbfOZy04K8guDMMmwUTH0vnvBpQB650icf2BlsThwG%2B5CBv2tPY70oAIiRcrrn%2Fugy8hB5ewNC8ndqRgj9oKPG%2BE2SrPhYfJQVYMxUiGH8PzsampSg9Q9bPlojm9%2FJt1lF2YZnayoCVgLdEQ9%2FGNJtYKY5iPGOde48lbN4mW9ta3qFWr3sfgrvGBizLFVoapc1A3m90ln382YurFy13HbLmLSJIqAljracSj0sV0qA0x78L31syuhlMU54EUcptXair3uJ4YGPoTrEqga83k0AfEt09PsR8MtTq5NO2EHlAUcm8mwLq%2BiCNj%2Bzih%2Ffs1R45t%2F4dlHQnFxj48wRqrzFXvsOVYEzYxnCfZsoqBxcP02ozVwSg%2B8Ee8tuOZKAm8KcMkRrN%2BZ3nwNoMyQmZiuu36j%2Bov57DHEuBMLPWwLwGOrABLNTtqYs%2BPzVy4ODgI2tEnb5g77hedOwHkkIx9jZXMkHCJ5N22tOZzsFkayViMbxF%2BmSoHUZD4cqiYu3Bry%2Fvq2kKW2vfjltKVhkD6Vr15n691973ZdkRaMJ8aV9upCUQD8%2FgPI3r0Jz0REJ4sd3piQ6hJeEh%2FAsPXlq4hzf7OpROhi7psE8gMVzsNq2iilK6Uph3eNR9qXiz3ctV82QBq7Bqxh9BGteUrlot8%2BjRlOk%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250122T003914Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWE6372GSGW%2F20250122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=84847f2375e608d3609ed1114fea19617c60bf6beaf96ae58e3bb622160567ce&abstractId=3159577)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c296da8",
   "metadata": {},
   "source": [
    "# Neural Network With Refitting\n",
    "Here we will be using expanding windows with 5 years in between each refitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f20f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for expanding window estimation\n",
    "cleandat_refit = cleandat  # Copy of clean data\n",
    "refit_period = 5  # Number of years between model refits\n",
    "fit_start = 1963  # Initial training start year\n",
    "fit_end = 1979  # Initial training end year\n",
    "nport = 5  # Number of portfolios to form\n",
    "\n",
    "# Lists to store results\n",
    "all_predictions = []  # Store all model predictions\n",
    "all_portdat = []  # Store all portfolio returns\n",
    "\n",
    "# Rolling window estimation\n",
    "for end_year in range(fit_end, 2030, refit_period):\n",
    "    # Get training data between 1963-1979\n",
    "    train_data = cleandat_refit.filter(\n",
    "        pl.col(\"date\").dt.year().is_between(fit_start, end_year)\n",
    "    )\n",
    "    X_train = train_data.select(pl.col(signal_list)).to_pandas().values\n",
    "    y_train = train_data.select(pl.col(\"ret\")).to_pandas().values.ravel()\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Initialize and train neural network\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(32, 16, 8),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=0.0001,\n",
    "        batch_size=10000,\n",
    "        learning_rate_init=0.01,\n",
    "        max_iter=100,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=5,\n",
    "        random_state=end_year,\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get test data for next period\n",
    "    test_data = cleandat_refit.filter(\n",
    "        pl.col(\"date\").dt.year().is_between(end_year + 1, end_year + refit_period)\n",
    "    )\n",
    "    if len(test_data) == 0:\n",
    "        continue\n",
    "\n",
    "    # Generate predictions on test data\n",
    "    X_test = test_data.select(pl.col(signal_list)).to_pandas().values\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    predictions = mlp.predict(X_test_scaled)\n",
    "\n",
    "    # Add predictions to test data\n",
    "    test_data = test_data.with_columns(pred=predictions).select(\n",
    "        [\"permno\", \"date\", \"pred\", \"ret\"]\n",
    "    )\n",
    "    all_predictions.append(test_data)\n",
    "\n",
    "    # Form portfolios based on predictions\n",
    "    preddat_refit = test_data.to_pandas()\n",
    "    preddat_refit[\"port\"] = preddat_refit.groupby(\"date\")[\"pred\"].transform(\n",
    "        port_sort, nport=nport\n",
    "    )\n",
    "\n",
    "    # Calculate portfolio returns\n",
    "    portdat_refit = (\n",
    "        preddat_refit.groupby([\"port\", \"date\"], observed=False)\n",
    "        .agg(ret=(\"ret\", \"mean\"), nstock=(\"permno\", \"nunique\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    all_portdat.append(portdat_refit)\n",
    "\n",
    "# Combine results\n",
    "all_predictions = pl.concat(all_predictions).select([\"permno\", \"date\", \"pred\", \"ret\"])\n",
    "portdat_refit = pd.concat(all_portdat, ignore_index=True)\n",
    "\n",
    "# Create subsamples for analysis\n",
    "samplength = 10\n",
    "\n",
    "portdat_refit[\"subsamp\"] = pd.cut(\n",
    "    portdat_refit[\"date\"].dt.year,\n",
    "    bins=range(1959, 2030, samplength),\n",
    "    labels=range(1959, 2029, samplength),\n",
    ")\n",
    "\n",
    "# Calculate summary statistics by portfolio and subsample\n",
    "portsum_refit = (\n",
    "    portdat_refit.groupby([\"port\", \"subsamp\"], observed=False)\n",
    "    .agg(\n",
    "        meanret=(\"ret\", \"mean\"),\n",
    "        vol=(\"ret\", \"std\"),\n",
    "        nmonth=(\"date\", \"nunique\"),\n",
    "        nstock=(\"nstock\", \"mean\"),\n",
    "        datemin=(\"date\", \"min\"),\n",
    "        datemax=(\"date\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "portsum_refit[\"meanret\"] = round(portsum_refit[\"meanret\"], 2)\n",
    "\n",
    "# Pivot results to wide format\n",
    "sumwide_refit = portsum_refit.pivot(\n",
    "    index=\"subsamp\", columns=\"port\", values=\"meanret\"\n",
    ").reset_index()\n",
    "sumwide_refit.columns = [\"subsamp\"] + [\n",
    "    f\"port_{col}\" for col in sumwide_refit.columns[1:]\n",
    "]\n",
    "\n",
    "# Calculate long-short portfolio returns\n",
    "sumwide_refit[\"5_minus_1\"] = sumwide_refit[\"port_5\"] - sumwide_refit[\"port_1\"]\n",
    "\n",
    "# Add date range for each subsample\n",
    "temp_refit = (\n",
    "    portsum_refit.groupby(\"subsamp\", observed=False)\n",
    "    .agg(datemin=(\"datemin\", \"min\"), datemax=(\"datemax\", \"max\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "sumwide_refit = pd.merge(temp_refit, sumwide_refit, on=\"subsamp\", how=\"left\")\n",
    "\n",
    "# Add the labels for each decade\n",
    "sumwide_refit[\"subsamp\"] = sumwide_refit[\"subsamp\"].map(\n",
    "    {\n",
    "        1959: \"groovy\",\n",
    "        1969: \"groovy (still)\",\n",
    "        1979: \"hair metal\",\n",
    "        1989: \"gangsta rap\",\n",
    "        1999: \"emo\",\n",
    "        2009: \"EDM\",\n",
    "        2019: \"TSwift\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Drop first two rows (groovy & groovy (still))\n",
    "# This is because we are refitting with expanding windows so predictions do not exis for the test data \n",
    "sumwide_refit = sumwide_refit.dropna()\n",
    "\n",
    "sumwide_refit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9884554",
   "metadata": {},
   "source": [
    "Better! Our model with added refitting to the training data is able to achieve better performance than the original neural network model. This is because the refitting process ensures that the models adapts to the growing training dataset, improving its ability to generalize. In our previous example, the model would only reflect the data from the initial training period, ignoring any additional information from the subsequent (still) groovy era. This model could probably even go on to explain why Metallica started to paint their fingernails black."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
